{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hijZzxSA8PzD"
   },
   "source": [
    "# Addresses\n",
    "\n",
    "ACHO QUE 250 COMO INPUT TA SUAVE PQ O MAIOR ENDEREçO QUE ESTOU PENSANDO TERIA 125 caracteres e se multiplicar por 2 pra colocar os espaços da 250 e input e output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v6aM8Cn6yCTW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encryption_key():\n",
    "    chars = string.ascii_letters + string.digits + string.punctuation\n",
    "    key = ''.join(random.choice(chars) for _ in range(20))  # Adjust length as needed\n",
    "    return key\n",
    "\n",
    "def encrypt_word(word, key):\n",
    "    encrypted_word = ''\n",
    "    for i in range(len(word)):\n",
    "        char_code = ord(word[i])\n",
    "        key_code = ord(key[i % len(key)])\n",
    "        encrypted_char_code = char_code ^ key_code % 128 + 32 # XOR operation\n",
    "        encrypted_word += chr(encrypted_char_code)\n",
    "    return encrypted_word\n",
    "\n",
    "def decrypt_word(encrypted_word, key):\n",
    "    decrypted_word = ''\n",
    "    for i in range(len(encrypted_word)):\n",
    "        encrypted_char_code = ord(encrypted_word[i])\n",
    "        key_code = ord(key[i % len(key)])\n",
    "        decrypted_char_code = encrypted_char_code ^ key_code % 128 + 32 # XOR operation\n",
    "        decrypted_word += chr(decrypted_char_code)\n",
    "    return decrypted_word\n",
    "\n",
    "# Simple encrypt function only to not write directly your password\n",
    "pwd = \"Ñãê·¼ëàç°½¯\"\n",
    "nni = 'Ä»µ¼°²'\n",
    "key = \"abcdef\"\n",
    "NNI=decrypt_word(nni, key)\n",
    "PWD=decrypt_word(pwd, key)\n",
    "\n",
    "\n",
    "# Put your NNI as well as your personal password HERE !\n",
    "proxy = {\n",
    "    \"http\": f\"http://{NNI}:{PWD}@vip-users.proxy.edf.fr:3131\",\n",
    "    \"https\": f\"http://{NNI}:PWD!@vip-users.proxy.edf.fr:3131\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B2Tu633_jaY-"
   },
   "outputs": [],
   "source": [
    "# Step 1: Download and extract the CSV file\n",
    "url = \"https://adresse.data.gouv.fr/data/ban/adresses/latest/csv-bal/adresses-01.csv.gz\"\n",
    "filename = \"adresses-01.csv.gz\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(url, proxies = proxy)\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Unzip the file\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(\"adresses-01.csv\", 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(\"adresses-01.csv\", sep=';', encoding='utf-8', low_memory=False)\n",
    "df['numero_insee'] = df.apply(lambda x: x['cle_interop'].split('_')[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "wZX9Z9yQvweJ",
    "outputId": "6429c1b3-e31b-422e-d641-7c1adbc0c311"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Combinations:  58%|█████████████████████████████████▊                        | 7/12 [00:47<00:33,  6.74s/it]\n"
     ]
    }
   ],
   "source": [
    "combinations = [['numero', 'suffixe', 'voie_nom', 'commune_nom'],\n",
    "                ['suffixe', 'voie_nom', 'commune_nom'],\n",
    "                ['voie_nom', 'commune_nom'],\n",
    "                ['voie_nom'],\n",
    "                ['numero', 'voie_nom', 'commune_nom'],\n",
    "                ['numero', 'voie_nom'],\n",
    "                ['numero', 'suffixe', 'voie_nom', 'numero_insee', 'commune_nom'],\n",
    "                ['suffixe', 'voie_nom', 'numero_insee', 'commune_nom'],\n",
    "                ['voie_nom', 'numero_insee', 'commune_nom'],\n",
    "                ['voie_nom', 'numero_insee'],\n",
    "                ['numero', 'voie_nom', 'numero_insee', 'commune_nom'],\n",
    "                ['numero', 'voie_nom', 'numero_insee'],\n",
    "                ]\n",
    "\n",
    "def generate_dataset(df, combinations):\n",
    "    dataset = pd.DataFrame()\n",
    "    for i, combination in enumerate(tqdm(combinations, total=len(combinations), desc=\"Processing Combinations\")):\n",
    "        dataset[f'{i}_spaced'] = df.apply(lambda row: ' '.join(''.join([str(row[col]) if pd.notna(row[col]) else '' for col in combination]).replace(' ', '')), axis=1)\n",
    "        dataset[f'{i}_correct'] = df.apply(lambda row: ' '.join([str(row[col]) if pd.notna(row[col]) else '' for col in combination]),  axis=1)\n",
    "    return dataset\n",
    "dataset = generate_dataset(df, combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aydy_ZWI2gkg"
   },
   "outputs": [],
   "source": [
    "def tokens(S):\n",
    "    french_chars = 'àâçéèêëîïôùûüÿœæ'\n",
    "    all_chars = string.ascii_letters + string.digits + string.punctuation + french_chars + ' '\n",
    "    \n",
    "    char2idx = {char: 5*(idx+1) for idx, char in enumerate(all_chars)}\n",
    "    idx2char = {}\n",
    "    for key, value in char2idx.items():\n",
    "        idx2char[value] = key\n",
    "\n",
    "    return char2idx, idx2char\n",
    "\n",
    "# Function to convert text to sequences of indices\n",
    "def text_to_sequence(text):\n",
    "    char2idx, idx2char = tokens()\n",
    "    return [char2idx[char] for char in text if char in char2idx]\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    char2idx, idx2char = tokens()\n",
    "    \n",
    "#     for i in sequence:\n",
    "#         if i in idx2char:\n",
    "#             print(f\"{idx2char[i]}\", end ='')\n",
    "#         else:\n",
    "#             print(f\" \", end ='')\n",
    "    \n",
    "    text = ''.join([idx2char[idx] if idx in idx2char else idx2char[len(idx2char)] for idx in sequence])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrE5u6648pkT"
   },
   "outputs": [],
   "source": [
    "# Ensure text_to_sequence function is compatible with GPU\n",
    "def tokenize_text_column(column_texts):\n",
    "    # Tokenize a list of texts (one column) in parallel\n",
    "    return [text_to_sequence(text) for text in column_texts]\n",
    "\n",
    "def tokenize_dataset_parallel(dataset, combinations, max_len = 250):\n",
    "\n",
    "    char2idx, _ = tokens()\n",
    "    input_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Using joblib for parallel processing of each combination\n",
    "    for i in tqdm(range(len(combinations)), total=len(combinations), desc=\"Processing Tokenized Inputs/Outputs\"):\n",
    "        label_column_texts = dataset[f'{i}_correct'].tolist()\n",
    "        input_column_texts = dataset[f'{i}_spaced'].tolist()\n",
    "\n",
    "        # Tokenize in parallel for each column of texts\n",
    "        input_data_column = Parallel(n_jobs=-1)(delayed(text_to_sequence)(text) for text in input_column_texts)\n",
    "        labels_column = Parallel(n_jobs=-1)(delayed(text_to_sequence)(text) for text in label_column_texts)\n",
    "\n",
    "        # Collect tokenized data\n",
    "        input_data.append(input_data_column)\n",
    "        labels.append(labels_column)\n",
    "\n",
    "    # Pad the sequences to the maximum length\n",
    "    input_data = [[seq + [char2idx[' ']] * (max_len - len(seq)) for seq in sublist] for sublist in input_data]\n",
    "    labels = [[seq + [char2idx[' ']] * (max_len - len(seq)) for seq in sublist] for sublist in labels]\n",
    "\n",
    "    # Convert to PyTorch tensors and move to GPU\n",
    "    input_data = torch.tensor(input_data, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    input_data = input_data.reshape(-1, input_data.size(2))\n",
    "    labels = labels.reshape(-1, labels.size(2))\n",
    "\n",
    "    return input_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhWivaTa3zHo",
    "outputId": "70bf2f61-2fdc-4c88-dfc4-b6345ea29550"
   },
   "outputs": [],
   "source": [
    "input_data, labels = tokenize_dataset_parallel(dataset, combinations, max_len = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8M1wc5LvrdC"
   },
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, neurons, output_size, depth):\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, neurons),\n",
    "            nn.Tanh(),\n",
    "            nn.Sequential(*[nn.Linear(neurons, neurons) for _ in range(depth-1)]),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(neurons, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KORAIlaW3YUd"
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "input_size = 250\n",
    "output_size = 250\n",
    "neurons = 80\n",
    "depth = 10\n",
    "model = FNN(input_size, neurons, output_size, depth)\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 512\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMm34LGrV2xl"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--------------------Epoch: {epoch}--------------------\")\n",
    "    # Loop over the dataset in batches\n",
    "    for i in range(0, input_data.shape[0], batch_size):\n",
    "        # Get the batch data\n",
    "        batch_input = input_data[i:i + batch_size]  # Slicing for batch input\n",
    "        batch_labels = labels[i:i + batch_size]  # Slicing for batch labels\n",
    "\n",
    "        # Ensure the current batch is not empty\n",
    "        if batch_input.shape[0] == 0 or batch_labels.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # Here, you would run your model on the current batch\n",
    "        outputs = model(batch_input.float())  # Forward pass\n",
    "    \n",
    "        loss = criterion(outputs, batch_labels.float())  # Compute loss\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%999 == 0:\n",
    "            print(f'Processed batch {i // batch_size + 1} with loss: {loss.item()}')\n",
    "torch.save(model.state_dict(), './address_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sample_addresses = [\"1 r u e d e l a p a i x\", \"2 a v e n u e d e s c h a m p s - é l y s é e s\"]\n",
    "\n",
    "max_len = 250\n",
    "char2idx, idx2char = tokens()\n",
    "seq = text_to_sequence(sample_addresses[0])\n",
    "seq = seq + [len(char2idx) + 1] * (max_len - len(text_to_sequence(sample_addresses[0]))) \n",
    "inp = torch.tensor(seq, dtype=torch.float)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predicted_addresses = torch.round(model(inp.to(device))).detach().cpu().numpy().astype(int)\n",
    "\n",
    "print(predicted_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_text(predicted_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx, idx2char = tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
